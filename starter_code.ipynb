{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 搭建一个简单的问答系统 （Building a Simple QA System）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次项目的目标是搭建一个基于检索式的简易的问答系统，这是一个最经典的方法也是最有效的方法。  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检索式的问答系统\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 项目中涉及到的任务描述\n",
    "问答系统看似简单，但其中涉及到的内容比较多。 在这里先做一个简单的解释，总体来讲，我们即将要搭建的模块包括：\n",
    "\n",
    "- 文本的读取： 需要从相应的文件里读取```(问题，答案)```\n",
    "- 文本预处理： 清洗文本很重要，需要涉及到```停用词过滤```等工作\n",
    "- 文本的表示： 如果表示一个句子是非常核心的问题，这里会涉及到```tf-idf```, ```Glove```以及```BERT Embedding```\n",
    "- 文本相似度匹配： 在基于检索式系统中一个核心的部分是计算文本之间的```相似度```，从而选择相似度最高的问题然后返回这些问题的答案\n",
    "- 倒排表： 为了加速搜索速度，我们需要设计```倒排表```来存储每一个词与出现的文本\n",
    "- 词义匹配：直接使用倒排表会忽略到一些意思上相近但不完全一样的单词，我们需要做这部分的处理。我们需要提前构建好```相似的单词```然后搜索阶段使用\n",
    "- 拼写纠错：我们不能保证用户输入的准确，所以第一步需要做用户输入检查，如果发现用户拼错了，我们需要及时在后台改正，然后按照修改后的在库里面搜索\n",
    "- 文档的排序： 最后返回结果的排序根据文档之间```余弦相似度```有关，同时也跟倒排表中匹配的单词有关\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 项目中需要的数据：\n",
    "1. ```dev-v2.0.json```: 这个数据包含了问题和答案的pair， 但是以JSON格式存在，需要编写parser来提取出里面的问题和答案。 \n",
    "2. ```glove.6B```: 这个文件需要从网上下载，下载地址为：https://nlp.stanford.edu/projects/glove/， 请使用d=200的词向量\n",
    "3. ```spell-errors.txt``` 这个文件主要用来编写拼写纠错模块。 文件中第一列为正确的单词，之后列出来的单词都是常见的错误写法。 但这里需要注意的一点是我们没有给出他们之间的概率，也就是p(错误|正确），所以我们可以认为每一种类型的错误都是```同等概率```\n",
    "4. ```vocab.txt``` 这里列了几万个英文常见的单词，可以用这个词库来验证是否有些单词被拼错\n",
    "5. ```testdata.txt``` 这里搜集了一些测试数据，可以用来测试自己的spell corrector。这个文件只是用来测试自己的程序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本次项目中，你将会用到以下几个工具：\n",
    "- ```sklearn```。具体安装请见：http://scikit-learn.org/stable/install.html  sklearn包含了各类机器学习算法和数据处理工具，包括本项目需要使用的词袋模型，均可以在sklearn工具包中找得到。 \n",
    "- ```jieba```，用来做分词。具体使用方法请见 https://github.com/fxsjy/jieba\n",
    "- ```bert embedding```: https://github.com/imgarylai/bert-embedding\n",
    "- ```nltk```：https://www.nltk.org/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一部分：对于训练数据的处理：读取文件和预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```文本的读取```： 需要从文本中读取数据，此处需要读取的文件是```dev-v2.0.json```，并把读取的文件存入一个列表里（list）\n",
    "- ```文本预处理```： 对于问题本身需要做一些停用词过滤等文本方面的处理\n",
    "- ```可视化分析```： 对于给定的样本数据，做一些可视化分析来更好地理解数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1节： 文本的读取\n",
    "把给定的文本数据读入到```qlist```和```alist```当中，这两个分别是列表，其中```qlist```是问题的列表，```alist```是对应的答案列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize # 分词\n",
    "from nltk.stem import PorterStemmer ## 词干提取 \n",
    "from nltk.corpus import reuters\n",
    "from collections import Counter,defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import heapq\n",
    "import scipy.sparse\n",
    "\n",
    "from queue import PriorityQueue\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file_path):\n",
    "    \"\"\"\n",
    "    读取给定的语料库，并把问题列表和答案列表分别写入到 qlist, alist 里面。 在此过程中，不用对字符换做任何的处理（这部分需要在 Part 2.3里处理）\n",
    "    qlist = [\"问题1\"， “问题2”， “问题3” ....]\n",
    "    alist = [\"答案1\", \"答案2\", \"答案3\" ....]\n",
    "    务必要让每一个问题和答案对应起来（下标位置一致）\n",
    "    \"\"\"\n",
    "    # TODO 需要完成的代码部分 ...\n",
    "    \n",
    "    with open(file_path, 'r') as path:\n",
    "        json_data = json.load(path)\n",
    "        #print(json_data)\n",
    "        \n",
    "    qlist = [] # 问题列表\n",
    "    alist = [] # 答案列表\n",
    "    \n",
    "    data = json_data['data']\n",
    "    \n",
    "    for eachdata in data:\n",
    "        for eachqas in eachdata['paragraphs']:\n",
    "            for qa in eachqas['qas']:\n",
    "                 if(len(qa['answers']))>0: # 有很多问题没有答案，所以需要判断去掉没有答案的数据=\n",
    "                    qlist.append(qa['question'])\n",
    "                    alist.append(qa['answers'][0]['text'])\n",
    "    assert len(qlist) == len(alist)  # 确保长度一样\n",
    "    return qlist, alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qlist=['When did Beyonce start becoming popular?','What areas did Beyonce compete in when she was growing up?',...]\n",
    "# alist = ['in the late 1990s','singing and dancing',...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlist, alist = read_corpus('train-v2.0.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 理解数据（可视化分析/统计信息）\n",
    "对数据的理解是任何AI工作的第一步， 需要对数据有个比较直观的认识。在这里，简单地统计一下：\n",
    "\n",
    "- 在```qlist```出现的总单词个数\n",
    "- 按照词频画一个```histogram``` plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词\n",
    "def cut(input_list):\n",
    "    list_new = []\n",
    "    for q in input_list:\n",
    "        list_new.append(q.replace('?','').split(' '))\n",
    "    return list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_one_sentence(sentence):\n",
    "    \"\"\"对单个句子进行分词\"\"\"\n",
    "    return sentence.replace('?','').split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共出现了 903411 个单词\n",
      "共有 51979 个不同的单词\n"
     ]
    }
   ],
   "source": [
    "# TODO: 统计一下在qlist中总共出现了多少个单词？ 总共出现了多少个不同的单词(unique word)？\n",
    "#       这里需要做简单的分词，对于英文我们根据空格来分词即可，其他过滤暂不考虑（只需分词）\n",
    "\n",
    "qlist_new = [q for l in cut(qlist) for q in l] # 分词\n",
    "dif_word_total = len(qlist_new) # 所有单词的个数\n",
    "\n",
    "word_dict = Counter(qlist_new) # Counter({单词：词频})\n",
    "word_total = len(dict(word_dict)) \n",
    "word_total_unique = [word for word in dict(word_dict)]  # 单词\n",
    "\n",
    "print (\"一共出现了 %d 个单词\"%dif_word_total) # 903411\n",
    "print (\"共有 %d 个不同的单词\"%word_total) #51979 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: 统计一下qlist中出现1次，2次，3次... 出现的单词个数， 然后画一个plot. \n",
    "# 这里的x轴是单词出现的次数（1，2，3，..)， y轴是单词个数。\n",
    "#       从左到右分别是 出现1次的单词数，出现2次的单词数，出现3次的单词数... \n",
    "\n",
    "y = []\n",
    "for i in word_dict: # word_dict=Counter({单词：词频})\n",
    "    y.append(word_dict[i]) # 词频\n",
    "\n",
    "plt.plot(sorted(y,reverse=True)[50:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO： 从上面的图中能观察到什么样的现象？ 这样的一个图的形状跟一个非常著名的函数形状很类似，能所出此定理吗？ \n",
    "#       hint: [XXX]'s law\n",
    "# \n",
    "# Zipf's law\n",
    "#从上图可以看出词库里面只有一部分单词经常出现，绝大部分的单词不怎么出现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.3 文本预处理\n",
    "此部分需要做文本方面的处理。 以下是可以用到的一些方法：\n",
    "\n",
    "- 1. 停用词过滤 （去网上搜一下 \"english stop words list\"，会出现很多包含停用词库的网页，或者直接使用NLTK自带的）   \n",
    "- 2. 转换成lower_case： 这是一个基本的操作   \n",
    "- 3. 去掉一些无用的符号： 比如连续的感叹号！！！， 或者一些奇怪的单词。\n",
    "- 4. 去掉出现频率很低的词：比如出现次数少于10,20.... （想一下如何选择阈值）\n",
    "- 5. 对于数字的处理： 分词完只有有些单词可能就是数字比如44，415，把所有这些数字都看成是一个单词，这个新的单词我们可以定义为 \"#number\"\n",
    "- 6. lemmazation： 在这里不要使用stemming， 因为stemming的结果有可能不是valid word。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 低频词库\n",
    "low_frequency_words = []\n",
    "for (k,v) in  word_dict.items():\n",
    "    if v < 2:\n",
    "        low_frequency_words.append(k)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO： 需要做文本方面的处理。 从上述几个常用的方法中选择合适的方法给qlist做预处理（不一定要按照上面的顺序，不一定要全部使用）\n",
    "\n",
    "def text_preprocessing(input_list):\n",
    "    \"\"\"预处理\"\"\"\n",
    "    \n",
    "    stop_words = set(stopwords.words('english')) # 停用词\n",
    "    stemmer = PorterStemmer() # 词干提取\n",
    "    input_list = cut(input_list)  # 分词  \n",
    "    new_list = [] #保存处理完的qlist\\alist\n",
    "    \n",
    "    for l in input_list:\n",
    "        l_list = '' # 保存句子\n",
    "        for word in l:\n",
    "            word = word.lower()      # 1.转换小写\n",
    "            word = stemmer.stem(word) # 2.词干提取\n",
    "            word = ''.join(c for c in word if c not in string.punctuation)  # 3.去除所有标点符号\n",
    "            \n",
    "            if word.isdigit(): # 4. 处理数字\n",
    "                word = word.replace(word,'#number')\n",
    "            \n",
    "            if word not in stop_words and word not in low_frequency_words: # 5.去停用词 6.过滤低频词\n",
    "                l_list += word + ' '\n",
    "        new_list.append(l_list)\n",
    "    return new_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "qlist, alist = read_corpus('train-v2.0.json') # 原问题列表 答案列表\n",
    "qlist = text_preprocessing(qlist)   # 预处理后的问题列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qlist = ['beyonc start becom popular ','area beyonc compet wa grow up ',...] 预处理后"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二部分： 文本的表示\n",
    "当我们做完必要的文本处理之后就需要想办法表示文本了，这里有几种方式\n",
    "\n",
    "- 1. 使用```tf-idf vector```\n",
    "- 2. 使用embedding技术如```word2vec```, ```bert embedding```等\n",
    "\n",
    "下面我们分别提取这三个特征来做对比。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 使用tf-idf表示向量\n",
    "把```qlist```中的每一个问题的字符串转换成```tf-idf```向量, 转换之后的结果存储在```X```矩阵里。 ``X``的大小是： ``N* D``的矩阵。 这里``N``是问题的个数（样本个数），\n",
    "``D``是词典库的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "import numpy as np\n",
    "\n",
    "def computeTF(vocab,c):\n",
    "    #计算每次词的词频\n",
    "    #vocabCount已经统计好的每词的次数\n",
    "    #c是统计好的总次数\n",
    "    TF = np.ones(len(vocab))\n",
    "    word2id = dict()\n",
    "    id2word = dict()\n",
    "    for word,fre in vocab.items():\n",
    "        TF[len(word2id)] = 1.0 * fre / c\n",
    "        id2word[len(word2id)] = word\n",
    "        word2id[word] = len(word2id)\n",
    "    return TF,word2id,id2word\n",
    "\n",
    "def computeIDF(word2id,qlist):\n",
    "    #IDF计算，没有类别，以句子为一个类\n",
    "    IDF = np.ones(len(word2id))\n",
    "    for q in qlist:\n",
    "        words = set(q.strip().split())\n",
    "        for w in words:\n",
    "            IDF[word2id[w]] += 1\n",
    "    IDF /= len(qlist)\n",
    "    IDF = -1.0 * np.log2(IDF)\n",
    "    return IDF\n",
    "\n",
    "def computeSentenceEach(sentence,tfidf,word2id):\n",
    "    #给定句子，计算句子TF-IDF\n",
    "    #tfidf是一个1*M的矩阵,M为词表大小\n",
    "    #不在词表中的词不统计\n",
    "    sentence_tfidf = np.zeros(len(word2id))\n",
    "    for w in sentence.strip().split(' '):\n",
    "        if w not in word2id:\n",
    "            continue\n",
    "        sentence_tfidf[word2id[w]] = tfidf[word2id[w]]\n",
    "    return sentence_tfidf\n",
    "\n",
    "def computeSentence(qlist,word2id,tfidf):\n",
    "    #对所有句子分别求tfidf\n",
    "    X_tfidf = np.zeros((len(qlist),len(word2id)))\n",
    "    for i,q in enumerate(qlist):\n",
    "        X_tfidf[i] = computeSentenceEach(q,tfidf,word2id)\n",
    "        #print(X_tfidf[i])\n",
    "    return X_tfidf\n",
    "\n",
    "TF,word2id,id2word = computeTF(vocab_count,count)\n",
    "print(len(word2id))\n",
    "IDF = computeIDF(word2id,qlist)\n",
    "#用TF，IDF计算最终的tf-idf\n",
    "vectorizer = np.multiply(TF,IDF)# 定义一个tf-idf的vectorizer\n",
    "#print(vectorizer)\n",
    "X_tfidf =  computeSentence(qlist,word2id,vectorizer) # 结果存放在X矩阵里\n",
    "print(X_tfidf[0])\n",
    "print(X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 使用wordvec + average pooling\n",
    "词向量方面需要下载： https://nlp.stanford.edu/projects/glove/ （请下载``glove.6B.zip``），并使用``d=200``的词向量（200维）。国外网址如果很慢，可以在百度上搜索国内服务器上的。 每个词向量获取完之后，即可以得到一个句子的向量。 我们通过``average pooling``来实现句子的向量。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 基于Glove向量获取句子向量\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "def loadEmbedding(filename):\n",
    "    #加载glove模型，转为word2vec，再加载word2vec模型\n",
    "    word2vec_temp_file ='word2vec_temp.txt'\n",
    "    glove2word2vec(filename,word2vec_temp_file)\n",
    "    model =KeyedVectors.load_word2vec_format(word2vec_temp_file)\n",
    "    return model\n",
    "\n",
    "def computeGloveSentenceEach(sentence,embedding):\n",
    "    #查找句子中每个词的embedding,将所有embedding进行加和求均值\n",
    "    emb = np.zeros(200)\n",
    "    words =sentence.strip().split(\" \")\n",
    "    for w in words:\n",
    "        if w not  in embedding:\n",
    "            #没有lookup的即为unkonwn\n",
    "            w = 'unknown'\n",
    "        emb +=embedding(w)\n",
    "    return emb/len(words)\n",
    "\n",
    "def computeGloveSentence(qlist,embedding):\n",
    "    #对每一个句子尽心平均值的embedding\n",
    "    X_w2v =np.zeros(len(qlist),200)\n",
    "    for i,q in enumerate(qlist):\n",
    "        X_w2v[i] = computeGloveSentenceEach(q,embedding)\n",
    "        #print(X_w2v)\n",
    "    return X_w2v\n",
    "\n",
    "emb  = loadEmbedding('glove.6B.200d.txt') # 这是 D*H的矩阵，这里的D是词典库的大小， H是词向量的大小。 这里面我们给定的每个单词的词向量，\n",
    "        # 这需要从文本中读取\n",
    "\n",
    "X_w2v = computeGloveSentence(qlist,emb)   # 初始化完emb之后就可以对每一个句子来构建句子向量了，这个过程使用average pooling来实现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 使用BERT + average pooling\n",
    "最近流行的BERT也可以用来学出上下文相关的词向量（contex-aware embedding）， 在很多问题上得到了比较好的结果。在这里，我们不做任何的训练，而是直接使用已经训练好的BERT embedding。 具体如何训练BERT将在之后章节里体会到。 为了获取BERT-embedding，可以直接下载已经训练好的模型从而获得每一个单词的向量。可以从这里获取： https://github.com/imgarylai/bert-embedding , 请使用```bert_12_768_12```\t当然，你也可以从其他source获取也没问题，只要是合理的词向量。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_embedding import BertEmbedding\n",
    "import mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 基于BERT的句子向量计算\n",
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "sentence_embedding = np.ones((len(qlist),768))\n",
    "#加载Bert模型，model，dataset_name,须指定\n",
    "bert_embedding = BertEmbedding(model='bert_12_768_12', dataset_name='wiki_multilingual_cased')\n",
    "#查询所有句子的Bert  embedding\n",
    "#all_embedding = []\n",
    "#for q in qlist:\n",
    "#    all_embedding.append(bert_embedding([q],'sum'))\n",
    "all_embedding = bert_embedding(qlist,'sum')\n",
    "for i in range(len(all_embedding)):\n",
    "    #print(all_embedding[i][1])\n",
    "    sentence_embedding[i] = np.sum(all_embedding[i][1],axis = 0) / len(q.strip().split(' '))\n",
    "    if i == 0:\n",
    "        print(sentence_embedding[i])\n",
    "X_bert =  sentence_embedding # 每一个句子的向量结果存放在X_bert矩阵里。行数为句子的总个数，列数为一个句子embedding大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 第三部分： 相似度匹配以及搜索\n",
    "在这部分里，我们需要把用户每一个输入跟知识库里的每一个问题做一个相似度计算，从而得出最相似的问题。但对于这个问题，时间复杂度其实很高，所以我们需要结合倒排表来获取相似度最高的问题，从而获得答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 tf-idf + 余弦相似度\n",
    "我们可以直接基于计算出来的``tf-idf``向量，计算用户最新问题与库中存储的问题之间的相似度，从而选择相似度最高的问题的答案。这个方法的复杂度为``O(N)``， ``N``是库中问题的个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alist = np.array(alist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_results_tfidf_noindex(query):\n",
    "    # TODO 需要编写\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 query, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "    1. 对于用户的输入 query 首先做一系列的预处理(上面提到的方法)，然后再转换成tf-idf向量（利用上面的vectorizer)\n",
    "    2. 计算跟每个库里的问题之间的相似度\n",
    "    3. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "    input_q = text_preprocessing([query]) # 对输入的query问题进行预处理 \n",
    "    input_vec = vectorizer.transform(input_q) # 转为tfidf向量\n",
    "    res = cosine_similarity(input_vec,X)[0] # 计算query跟每个库里的问题之间的相似度\n",
    "    top_idxs = np.argsort(res)[-5:].tolist()  # top_idxs存放相似度最高的（存在qlist里的）问题的下标 np.argsort输出排序后的下标\n",
    "    return alist[top_idxs] # 返回相似度最高的问题对应的答案，作为TOP5答案    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 编写几个测试用例，并输出结果  \n",
    "# 给出问题 找出答案\n",
    "print (get_top_results_tfidf_noindex(\"Which airport was shut down?\"))\n",
    "print (get_top_results_tfidf_noindex(\"Which airport is closed?\"))\n",
    "print (get_top_results_tfidf_noindex(\"What government blocked aid after Cyclone Nargis?\"))\n",
    "print (get_top_results_tfidf_noindex(\"Which government stopped aid after Hurricane Nargis?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#结果\n",
    "# ['April 2006' 'newspapers'\n",
    "#  'aerodrome with facilities for flights to take off and land'\n",
    "#  'Chengdu Shuangliu International Airport'\n",
    "#  'Chengdu Shuangliu International Airport']\n",
    "# ['YYT' 'Bern Airport' 'Newburgh, New York'\n",
    "#  'aerodrome with facilities for flights to take off and land'\n",
    "#  'Plymouth City Airport']\n",
    "# ['three' 'the British government' '10 days' 'foreign aid' 'Myanmar']\n",
    "# ['10 days' 'war on terror' 'foreign aid' 'Isabel' 'Myanmar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你会发现上述的程序很慢，没错！ 是因为循环了所有库里的问题。为了优化这个过程，我们需要使用一种数据结构叫做```倒排表```。 使用倒排表我们可以把单词和出现这个单词的文档做关键。 之后假如要搜索包含某一个单词的文档，即可以非常快速的找出这些文档。 在这个QA系统上，我们首先使用倒排表来快速查找包含至少一个单词的文档，然后再进行余弦相似度的计算，即可以大大减少```时间复杂度```。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 倒排表的创建\n",
    "倒排表的创建其实很简单，最简单的方法就是循环所有的单词一遍，然后记录每一个单词所出现的文档，然后把这些文档的ID保存成list即可。我们可以定义一个类似于```hash_map```, 比如 ``inverted_index = {}``， 然后存放包含每一个关键词的文档出现在了什么位置，也就是，通过关键词的搜索首先来判断包含这些关键词的文档（比如出现至少一个），然后对于candidates问题做相似度比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 请创建倒排表\n",
    "inverted_idx = {}  # 定一个一个简单的倒排表，是一个map结构。 循环所有qlist一遍就可以.\n",
    "for i in range(len(qlist)): ##问题的个数\n",
    "    for word in qlist[i].split(' '): # 分词\n",
    "        if word in inverted_idx.keys(): # 如果单词在倒排表中\n",
    "            inverted_idx[word].append(i) # 添加标记\"问题1\"（相当于doc1）\n",
    "        else: # 单词不在倒排表中 ，直接赋值（相当于添加了键和值）\n",
    "            inverted_idx[word] = [i]\n",
    "\n",
    "for key in inverted_idx:\n",
    "    inverted_idx[key] = sorted(inverted_idx[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 语义相似度\n",
    "这里有一个问题还需要解决，就是语义的相似度。可以这么理解： 两个单词比如car, auto这两个单词长得不一样，但从语义上还是类似的。如果只是使用倒排表我们不能考虑到这些单词之间的相似度，这就导致如果我们搜索句子里包含了``car``, 则我们没法获取到包含auto的所有的文档。所以我们希望把这些信息也存下来。那这个问题如何解决呢？ 其实也不难，可以提前构建好相似度的关系，比如对于``car``这个单词，一开始就找好跟它意思上比较类似的单词比如top 10，这些都标记为``related words``。所以最后我们就可以创建一个保存``related words``的一个``map``. 比如调用``related_words['car']``即可以调取出跟``car``意思上相近的TOP 10的单词。 \n",
    "\n",
    "那这个``related_words``又如何构建呢？ 在这里我们仍然使用``Glove``向量，然后计算一下俩俩的相似度（余弦相似度）。之后对于每一个词，存储跟它最相近的top 10单词，最终结果保存在``related_words``里面。 这个计算需要发生在离线，因为计算量很大，复杂度为``O(V*V)``， V是单词的总数。 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 读取语义相关的单词\n",
    "def get_related_words(file):\n",
    "    #从预处理的相似词的文件加载相似词信息\n",
    "    #文件格式w1 w2 w3..w11,其中w1为原词，w2-w11为w1的相似词\n",
    "    related_words = {}\n",
    "    with codecs.open(file,'r','utf8') as Fin:\n",
    "        lines = Fin.readlines()\n",
    "    for line in lines:\n",
    "        words = line.strip().split(' ')\n",
    "        related_words[words[0]] = words[1:]\n",
    "    return related_words\n",
    "\n",
    "related_words = get_related_words('related_words.txt') # 直接放在文件夹的根目录下，不要修改此路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 利用倒排表搜索\n",
    "在这里，我们使用倒排表先获得一批候选问题，然后再通过余弦相似度做精准匹配，这样一来可以节省大量的时间。搜索过程分成两步：\n",
    "\n",
    "- 使用倒排表把候选问题全部提取出来。首先，对输入的新问题做分词等必要的预处理工作，然后对于句子里的每一个单词，从``related_words``里提取出跟它意思相近的top 10单词， 然后根据这些top词从倒排表里提取相关的文档，把所有的文档返回。 这部分可以放在下面的函数当中，也可以放在外部。\n",
    "- 然后针对于这些文档做余弦相似度的计算，最后排序并选出最好的答案。\n",
    "\n",
    "可以适当定义自定义函数，使得减少重复性代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue as Q\n",
    "def cosineSimilarity(vec1,vec2):\n",
    "    #定义余弦相似度\n",
    "    return np.dot(vec1,vec2.T)/(np.sqrt(np.sum(vec1**2))*np.sqrt(np.sum(vec2**2)))\n",
    "\n",
    "def getCandidate(query):\n",
    "    #根据查询句子中每个词所在的序号列表，求交集\n",
    "    searched =set()\n",
    "    for w in query.strip().split(' '):\n",
    "        if w not in word2id or w not in inverted_idx:\n",
    "            continue\n",
    "        if len(searched) ==0:\n",
    "            searched = searched(inverted_idx[w])\n",
    "        else:\n",
    "            searched = searched & set(inverted_idx[w])\n",
    "        #搜索相似词所在的列表\n",
    "        if w in related_words:\n",
    "            for similar in related_words[w]:\n",
    "                searched = searched & set(inverted_idx[similar])\n",
    "    return searched\n",
    "\n",
    "\n",
    "def get_top_results_tfidf(query):\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 query, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "    1. 利用倒排表来筛选 candidate （需要使用related_words).\n",
    "    2. 对于候选文档，计算跟输入问题之间的相似度\n",
    "    3. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "    top =5\n",
    "    query_tfidf = computeSentenceEach(query,vectorizer,word2id)\n",
    "    results =Q.PriorityQueue()\n",
    "    searched = getCandidate(query)\n",
    "    for candidate  in searched:\n",
    "        #计算candidate与querry的相似度\n",
    "        result = cosineSimilarity(query_tfidf,X_tfidf[candidate])\n",
    "        #优先级队列中保存相似度和对应的candidate序号\n",
    "        #-1保证降序\n",
    "        results.put((-1*result,candidate))\n",
    "    i=0\n",
    "    top_idxs = []  # top_idxs存放相似度最高的（存在qlist里的）问题的下表\n",
    "                   # hint: 利用priority queue来找出top results. 思考为什么可以这么做？\n",
    "    while i < top and not results:\n",
    "        top_idxs.append(results.get()[1])\n",
    "        i+=1\n",
    "\n",
    "    return np.array(alist)[top_idxs]  # 返回相似度最高的问题对应的答案，作为TOP5答案\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_results_bert(query):\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 query, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "    1. 利用倒排表来筛选 candidate （需要使用related_words).\n",
    "    2. 对于候选文档，计算跟输入问题之间的相似度\n",
    "    3. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "    top = 5\n",
    "    results = Q.PriorityQueue()\n",
    "    searched = getCandidate(query)\n",
    "    query_emb = np.sum(bert_embedding([query],'sum')[0][1],axis = 0) / len(query.strip().split())\n",
    "    for candidate in searched:\n",
    "        result = cosineSimilarity(query_emb,X_bert[candidate])\n",
    "        results.put((-1*result,candidate))\n",
    "\n",
    "    top_idxs = []  # top_idxs存放相似度最高的（存在qlist里的）问题的下表\n",
    "                   # hint: 利用priority queue来找出top results. 思考为什么可以这么做？\n",
    "    i = 0\n",
    "    while i< top and not results.empty():\n",
    "        top_idxs.append(results.get()[1])\n",
    "        i+=1\n",
    "\n",
    "    return np.array(alist)[top_idxs]  # 返回相似度最高的问题对应的答案，作为TOP5答案\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_top_results_w2v(query):\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 query, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "    1. 利用倒排表来筛选 candidate （需要使用related_words).\n",
    "    2. 对于候选文档，计算跟输入问题之间的相似度\n",
    "    3. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "    top =5\n",
    "    results = Q.PriorityQueue()\n",
    "    query_emb = computeGloveSentenceEach(query,emb)\n",
    "    searched = getCandidate(query)\n",
    "    for candidate in searched:\n",
    "        result = cosineSimilarity(query_emb,X_w2v[candidate])\n",
    "        results.put((-1*result,candidate))\n",
    "    top_idxs = []  # top_idxs存放相似度最高的（存在qlist里的）问题的下表\n",
    "                   # hint: 利用priority queue来找出top results. 思考为什么可以这么做？\n",
    "    i = 0\n",
    "    while i < top and not results:\n",
    "        top_idxs.append(results.get()[1])\n",
    "\n",
    "    return np.array(alist)[top_idxs]  # 返回相似度最高的问题对应的答案，作为TOP5答案"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 编写几个测试用例，并输出结果\n",
    "\n",
    "test_query1 = \"When did Beyonce start becoming popular\"\n",
    "#result:in the late 1990s\n",
    "test_query2 = \"What counted for more of the population change\"\n",
    "#result:births and deaths\n",
    "\n",
    "print (get_top_results_tfidf(test_query1))\n",
    "print (get_top_results_w2v(test_query1))\n",
    "print (get_top_results_bert(test_query1))\n",
    "\n",
    "print (get_top_results_tfidf(test_query2))\n",
    "print (get_top_results_w2v(test_query2))\n",
    "print (get_top_results_bert(test_query2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['June 24, 2003' 'Houston' '300 million' 'musical comedy'\n",
    "#  'Austin Powers in Goldmember']\n",
    "# [\"New York's Roseland Ballroom\" 'the ONE Campaign' 'six' 'beehive' '2011']\n",
    "# ['Bey Hive' 'lead singer' 'Joseph Broussard' 'modelling'\n",
    "#  'Feria hair color advertisements']\n",
    "# ['Spanish' 'Op. 58' 'Independent Women Part I' \"Marie d'Agoult\"\n",
    "#  'Crazy in Love']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 拼写纠错\n",
    "其实用户在输入问题的时候，不能期待他一定会输入正确，有可能输入的单词的拼写错误的。这个时候我们需要后台及时捕获拼写错误，并进行纠正，然后再通过修正之后的结果再跟库里的问题做匹配。这里我们需要实现一个简单的拼写纠错的代码，然后自动去修复错误的单词。\n",
    "\n",
    "这里使用的拼写纠错方法是课程里讲过的方法，就是使用noisy channel model。 我们回想一下它的表示：\n",
    "\n",
    "$c^* = \\text{argmax}_{c\\in candidates} ~~p(c|s) = \\text{argmax}_{c\\in candidates} ~~p(s|c)p(c)$\n",
    "\n",
    "这里的```candidates```指的是针对于错误的单词的候选集，这部分我们可以假定是通过edit_distance来获取的（比如生成跟当前的词距离为1/2的所有的valid 单词。 valid单词可以定义为存在词典里的单词。 ```c```代表的是正确的单词， ```s```代表的是用户错误拼写的单词。 所以我们的目的是要寻找出在``candidates``里让上述概率最大的正确写法``c``。 \n",
    "\n",
    "$p(s|c)$，这个概率我们可以通过历史数据来获得，也就是对于一个正确的单词$c$, 有百分之多少人把它写成了错误的形式1，形式2...  这部分的数据可以从``spell_errors.txt``里面找得到。但在这个文件里，我们并没有标记这个概率，所以可以使用uniform probability来表示。这个也叫做channel probability。\n",
    "\n",
    "$p(c)$，这一项代表的是语言模型，也就是假如我们把错误的$s$，改造成了$c$， 把它加入到当前的语句之后有多通顺？在本次项目里我们使用bigram来评估这个概率。 举个例子： 假如有两个候选 $c_1, c_2$， 然后我们希望分别计算出这个语言模型的概率。 由于我们使用的是``bigram``， 我们需要计算出两个概率，分别是当前词前面和后面词的``bigram``概率。 用一个例子来表示：\n",
    "\n",
    "给定： ``We are go to school tomorrow``， 对于这句话我们希望把中间的``go``替换成正确的形式，假如候选集里有个，分别是``going``, ``went``, 这时候我们分别对这俩计算如下的概率：\n",
    "$p(going|are)p(to|going)$和 $p(went|are)p(to|went)$， 然后把这个概率当做是$p(c)$的概率。 然后再跟``channel probability``结合给出最终的概率大小。\n",
    "\n",
    "那这里的$p(are|going)$这些bigram概率又如何计算呢？答案是训练一个语言模型！ 但训练一个语言模型需要一些文本数据，这个数据怎么找？ 在这次项目作业里我们会用到``nltk``自带的``reuters``的文本类数据来训练一个语言模型。当然，如果你有资源你也可以尝试其他更大的数据。最终目的就是计算出``bigram``概率。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 训练一个语言模型\n",
    "在这里，我们使用``nltk``自带的``reuters``数据来训练一个语言模型。 使用``add-one smoothing``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取语料库的数据\n",
    "categories = reuters.categories()\n",
    "corpus = reuters.sents(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个单词出现的次数\n",
    "term_counts = {}\n",
    "# 为了简单起见，只记录每个单词出现在前一个单词之后的次数\n",
    "bigram_term_counts = {}\n",
    "\n",
    "for doc in corpus:\n",
    "    # 每个 doc 就是一个句子\n",
    "    # 在句子开头加一个起始标识符，将开头第一个单词表述成“在句子开头”的次数\n",
    "    doc = ['<s>'] + doc\n",
    "    \n",
    "    for index in range(0, len(doc) - 1):\n",
    "        \n",
    "        term = doc[index]\n",
    "        \n",
    "        bigram_term = doc[index:index + 2]\n",
    "        bigram_term = '{b}|{a}'.format(a=bigram_term[0], b=bigram_term[1])\n",
    "        \n",
    "        if term in term_counts:\n",
    "            term_counts[term] += 1\n",
    "        else:\n",
    "            term_counts[term] = 1\n",
    "            \n",
    "        if bigram_term in bigram_term_counts:\n",
    "            bigram_term_counts[bigram_term] += 1\n",
    "        else:\n",
    "            bigram_term_counts[bigram_term] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 构建Channel Probs\n",
    "基于``spell_errors.txt``文件构建``channel probability``, 其中$channel[c][s]$表示正确的单词$c$被写错成$s$的概率。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 构建channel probability  \n",
    "\n",
    "channel = {}\n",
    "\n",
    "spell_error_dict = {}\n",
    "\n",
    "for line in open('spell-errors.txt'): # raining: rainning, raning \\n writings: writtings\n",
    "    item = line.split(\":\") #['raining', ' rainning, raning']\n",
    "    word = item[0].strip()  # raining\n",
    "    spell_error_list = [word.strip( )for word in item[1].strip().split(\",\")] #['rainning', 'raning']\n",
    "    spell_error_dict[word] = spell_error_list  # spell_error_dict={'raining': ['rainning', 'raning']}\n",
    "    \n",
    "    channel[word]={}\n",
    "    for spell_error in spell_error_list:\n",
    "        channel[word][spell_error]=1.0/len(spell_error_list) #{'raining': {'rainning': 0.5}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 根据错别字生成所有候选集合\n",
    "给定一个错误的单词，首先生成跟这个单词距离为1或者2的所有的候选集合。 这部分的代码我们在课程上也讲过，可以参考一下。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([line.rstrip() for line in open('vocab.txt')]) # 词典库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words): # 不在词库的词 就不要  只留写法上正确的单词。\n",
    "    return list(set(w for w in words if w in vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def edits1(word):\n",
    "\n",
    "    \"\"\"生成编辑距离为1的单词\"\"\" \n",
    "    # 1.insert 2. delete 3. replace\n",
    "    # appl: replace: bppl, cppl, aapl, abpl... \n",
    "    #       insert: bappl, cappl, abppl, acppl....\n",
    "    #       delete: ppl, apl, app\n",
    "    \n",
    "    # 假设使用26个字符\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz' \n",
    "    \n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word)+1)] #[('', 'appl'), ('a', 'ppl'), ('ap', 'pl'), ('app', 'l'), ('appl', '')]\n",
    "   \n",
    "    # insert操作\n",
    "    inserts = [L+c+R for L, R in splits for c in letters] #['aappl', 'bappl', 'cappl', 'dappl'...]\n",
    "    \n",
    "    # delete\n",
    "    deletes = [L+R[1:] for L,R in splits if R] #['ppl', 'apl', 'apl', 'app']\n",
    "    \n",
    "    # replace\n",
    "    replaces = [L+c+R[1:] for L,R in splits if R for c in letters]   # ['appl', 'bppl', 'cppl', 'dppl'\n",
    "    \n",
    "    candidates = set(inserts+deletes+replaces)   #{'appr', 'applu', 'appli', 'apprl', .} \n",
    "    \n",
    "    # 过滤掉不存在于词典库里面的单词\n",
    "    edit1_words = known(candidates)\n",
    "    \n",
    "    return edit1_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edits1(\"appl\")) #['apply', 'apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits2(word, edit1_words):\n",
    "    \"\"\" 生成编辑距离为2的单词\"\"\"\n",
    "    \n",
    "    edit2_words = set(e2 for e1 in edit1_words for e2 in edits1(e1))\n",
    "    \n",
    "    # 过滤掉不存在于词典库里面的单词\n",
    "    edit2_words = known(edit2_words)\n",
    "    \n",
    "    return edit2_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit1_words=edits1(\"appl\")\n",
    "print(edits2(\"appl\",edit1_words)) #['amply', 'apply', 'apples', 'ample', 'apple', 'aptly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(word):\n",
    "    # 基于拼写错误的单词，生成跟它的编辑距离为1或者2的单词，并通过词典库的过滤。\n",
    "    # 只留写法上正确的单词。\n",
    "    \n",
    "    edit1_words = edits1(word)     # 编辑距离为1的候选项\n",
    "    edit2_words = edits2(word, edit1_words)   # 编辑距离为2的候选项\n",
    "    \n",
    "    candidates = edit1_words + edit2_words\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_candidates(\"appl\"))\n",
    "#['apple', 'apply', 'apples', 'ample', 'apply', 'amply', 'apple', 'aptly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 给定一个输入，如果有错误需要纠正\n",
    "\n",
    "给定一个输入``query``, 如果这里有些单词是拼错的，就需要把它纠正过来。这部分的实现可以简单一点： 对于``query``分词，然后把分词后的每一个单词在词库里面搜一下，假设搜不到的话可以认为是拼写错误的! 人如果拼写错误了再通过``channel``和``bigram``来计算最适合的候选。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(term_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_corrector(line):\n",
    "    # 1. 首先做分词，然后把``line``表示成``tokens``\n",
    "    # 2. 循环每一token, 然后判断是否存在词库里。如果不存在就意味着是拼写错误的，需要修正。 \n",
    "    #    修正的过程就使用上述提到的``noisy channel model``, 然后从而找出最好的修正之后的结果。 \n",
    "    sentence_words = line.split()\n",
    "    # 找出错误的单词\n",
    "    new_sentence = \"\"\n",
    "    for index, word in enumerate(sentence_words):   \n",
    "        \n",
    "        word = word.rstrip('.').strip(',') # 去掉单词中出现的 , 和末尾的 . \n",
    "        if word not in vocab: # 找到拼写错误的单词\n",
    "            mis_word = word\n",
    "            # 生成针对拼写错误的候选词汇集\n",
    "            candidates = generate_candidates(mis_word)\n",
    "            \n",
    "            # 如果生成不出候选正确单词，说明不能通过编辑距离为 1 找到候选的正确单词，我们可以继续生成编辑距离为 2 的候选正确单词，这里暂不实现\n",
    "            if len(candidates) == 0:\n",
    "                continue\n",
    "                \n",
    "            scores = []\n",
    "            scores_dict = {}\n",
    "                \n",
    "            # 为每一个候选单词计算分数\n",
    "            # score = p(写错|正确)*p(正确)= log(p(写错|正确)) + log(p(正确))\n",
    "            for candi in candidates:\n",
    "                \n",
    "                score = 0.0\n",
    "                \n",
    "                # log(p(写错|正确))   channel[c][s]表示正确的单词c被写错成s的概率\n",
    "                if candi in channel and mis_word in channel[candi]:\n",
    "                    score += np.log(channel[candi][mis_word])\n",
    "                else:\n",
    "                    score += np.log(0.00000000001) # 简化处理\n",
    "                \n",
    "                # log(p(正确))  语言模型 用Bigram\n",
    "                # Bigram with Add-one Smoothing      \n",
    "                prev_word = sentence_words[index - 1] if index > 0 else '<s>'# 前一个单词Wi-1\n",
    "                target_word = candi# 当前单词\n",
    "                bigram_term_string = target_word + '|' + prev_word\n",
    "                \n",
    "                \n",
    "                #bigram_count 前后两个单词在一起的频次（bigram_term_counts['ASIAN EXPORTERS']=1）\n",
    "                if target_word in term_counts and bigram_term_string in bigram_term_counts:\n",
    "                    print(111)\n",
    "                    score += np.log((bigram_term_counts[bigram_term_string] + 1) / (term_counts[target_word] + V))\n",
    "                else:\n",
    "                    score += np.log(1.0 / V)\n",
    "                    \n",
    "                scores.append(score)\n",
    "                scores_dict[candi] = score\n",
    "                # 输出结果\n",
    "            max_index = scores.index(max(scores))\n",
    "            best_candicate = candidates[max_index]\n",
    "            # print(mis_word, '-->', best_candicate)\n",
    "            # print(scores_dict)\n",
    "\n",
    "            word=best_candicate\n",
    "        new_sentence += word + \" \"\n",
    "    return new_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  bigram_term_counts={'ASIAN|<s>': 4, 'EXPORTERS|ASIAN': 1, 'FEAR|EXPORTERS': 1, ...\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query1 = \"How many polonaises were published aftr Chopin died?\"  # 拼写错误的\n",
    "\n",
    "test_query1 = spell_corrector(test_query1)\n",
    "print(test_query1) #How many polonaise were published after Chopin died \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 基于拼写纠错算法，实现用户输入自动矫正\n",
    "首先有了用户的输入``query``， 然后做必要的处理把句子转换成tokens的形状，然后对于每一个token比较是否是valid, 如果不是的话就进行下面的修正过程。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query1 = \"How many polonaises were published aftr Chopin died?\"  # 拼写错误的\n",
    "test_query2 = \"What ar doctors part of?\"  # 拼写错误的\n",
    "\n",
    "test_query1 = spell_corrector(test_query1)\n",
    "test_query2 = spell_corrector(test_query2)\n",
    "\n",
    "print (get_top_results_tfidf(test_query1))\n",
    "print (get_top_results_w2v(test_query1))\n",
    "print (get_top_results_bert(test_query1))\n",
    "\n",
    "print (get_top_results_tfidf(test_query2))\n",
    "print (get_top_results_w2v(test_query2))\n",
    "print (get_top_results_bert(test_query2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 附录 \n",
    "在本次项目中我们实现了一个简易的问答系统。基于这个项目，我们其实可以有很多方面的延伸。\n",
    "- 在这里，我们使用文本向量之间的余弦相似度作为了一个标准。但实际上，我们也可以基于基于包含关键词的情况来给一定的权重。比如一个单词跟related word有多相似，越相似就意味着相似度更高，权重也会更大。 \n",
    "- 另外 ，除了根据词向量去寻找``related words``也可以提前定义好同义词库，但这个需要大量的人力成本。 \n",
    "- 在这里，我们直接返回了问题的答案。 但在理想情况下，我们还是希望通过问题的种类来返回最合适的答案。 比如一个用户问：“明天北京的天气是多少？”， 那这个问题的答案其实是一个具体的温度（其实也叫做实体），所以需要在答案的基础上做进一步的抽取。这项技术其实是跟信息抽取相关的。 \n",
    "- 对于词向量，我们只是使用了``average pooling``， 除了average pooling，我们也还有其他的经典的方法直接去学出一个句子的向量。\n",
    "- 短文的相似度分析一直是业界和学术界一个具有挑战性的问题。在这里我们使用尽可能多的同义词来提升系统的性能。但除了这种简单的方法，可以尝试其他的方法比如WMD，或者适当结合parsing相关的知识点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}